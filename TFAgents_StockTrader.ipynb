{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VVasanth/RL_StockTrader-TFAgents/blob/main/TFAgents_StockTrader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48L2GKE9Yuof",
        "outputId": "ef0f42a5-f433-4c05-f409-bd9aefe60562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym>=0.21.0\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[K     |████████████████████████████████| 626 kB 11.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.3.0)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.21.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.21.0) (4.2.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701375 sha256=ccca6736481297852ac0f5c154ea2572050c0218c5a03ffd6e93ae3bdb89d200\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/33/04/6723848e46f0f1ebe794bb329b7c761c3329a0d7ffade99da7\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.23.1 gym-notices-0.0.6\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 85 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.0.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (4.11.3)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (0.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.17.0->tf-agents) (3.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Installing collected packages: pygame, tf-agents\n",
            "Successfully installed pygame-2.1.0 tf-agents-0.12.1\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [942 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,167 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [909 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,732 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,496 kB]\n",
            "Fetched 11.8 MB in 3s (4,107 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 0s (8,920 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155514 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting imageio==2.4.0\n",
            "  Downloading imageio-2.4.0.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-py3-none-any.whl size=3303895 sha256=cb02deec4d3b32ada3c246b383c8d40a13f3a5a6070964d622cf0060f43a0835\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/de/2f/6c5a75120d68a2c3138120c8d0ce1c6f9483a4b96307986bf2\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed imageio-2.4.0\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.2.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.23.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Collecting dm-reverb~=0.7.0\n",
            "  Downloading dm_reverb-0.7.2-cp37-cp37m-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (0.1.7)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (0.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.17.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.44.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n",
            "Installing collected packages: tf-estimator-nightly, dm-reverb\n",
            "Successfully installed dm-reverb-0.7.2 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gym>=0.21.0\"\n",
        "!pip install tf-agents\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9INYihrmlmWv"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import itertools\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import numpy as np\n",
        "\n",
        "class StockTraderEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, data, initInvestment=20000):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=26, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(7,), dtype=np.float64, name='observation')\n",
        "        self.stock_price_history = data\n",
        "        self.init_investment = initInvestment\n",
        "        self.state = 0\n",
        "        self.episode_ended = False\n",
        "        self.stock_owned = None\n",
        "        self.stock_price = None\n",
        "        self.cash_in_hand = None\n",
        "        self.n_stock = 3\n",
        "        self.n_step = self.stock_price_history.shape[0]\n",
        "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
        "#[[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 1, 0], [0, 1, 1], [0, 1, 2], [0, 2, 0], [0, 2, 1], [0, 2, 2], [1, 0, 0], [1, 0, 1], [1, 0, 2], [1, 1, 0],\n",
        "#[1, 1, 1], [1, 1, 2], [1, 2, 0], [1, 2, 1], [1, 2, 2], [2, 0, 0], [2, 0, 1], [2, 0, 2], [2, 1, 0], [2, 1, 1], [2, 1, 2], [2, 2, 0], [2, 2, 1], [2, 2, 2]]\n",
        "        self.action_space = np.arange(3 ** self.n_stock)\n",
        "        ###[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]##np.ndarray\n",
        "        self.scaler = None\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def set_scaler(self, scaler):\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def get_obs(self):\n",
        "        obs = np.empty(7)\n",
        "        obs[:self.n_stock] = self.stock_owned\n",
        "        obs[self.n_stock:2 * self.n_stock] = self.stock_price\n",
        "        obs[-1] = self.cash_in_hand\n",
        "        if self.scaler != None:\n",
        "            obs = self.scaler.transform([obs]).flatten()\n",
        "        return obs\n",
        "\n",
        "    def get_val(self):\n",
        "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = 0\n",
        "        self.episode_ended = False\n",
        "        self.stock_owned = np.zeros(self.n_stock)\n",
        "        self.stock_price = self.stock_price_history[self.state]\n",
        "        self.cash_in_hand = self.init_investment\n",
        "        return ts.restart(self.get_obs())\n",
        "\n",
        "    def _step(self, action):\n",
        "\n",
        "        if self.episode_ended:\n",
        "            # The last action ended the episode. Ignore the current action and start\n",
        "            # a new episode.\n",
        "            return self._reset()\n",
        "\n",
        "        # get current value before performing the action\n",
        "        prev_val = self.get_val()\n",
        "\n",
        "        self.stock_price = self.stock_price_history[self.state]\n",
        "\n",
        "        # update price, i.e. go to the next day\n",
        "        self.state += 1\n",
        "\n",
        "        # perform the trade\n",
        "        self.trade(action)\n",
        "\n",
        "        # get the new value after taking the action\n",
        "        cur_val = self.get_val()\n",
        "\n",
        "        # reward is the increase in porfolio value\n",
        "        reward = cur_val - prev_val\n",
        "\n",
        "        # done if we have run out of data\n",
        "        done = self.state == self.n_step - 1\n",
        "\n",
        "        # store the current value of the portfolio here\n",
        "        info = {'cur_val': self.state}\n",
        "\n",
        "        # conform to the Gym API\n",
        "        # return self._get_obs(), reward, done, info\n",
        "        if (done != True):\n",
        "            return ts.transition(self.get_obs(), reward=reward, discount=1.0)\n",
        "        else:\n",
        "            self.episode_ended = True\n",
        "            return ts.termination(self.get_obs(), reward)\n",
        "\n",
        "    def trade(self, action):\n",
        "        # index the action we want to perform\n",
        "        # 0 = sell\n",
        "        # 1 = hold\n",
        "        # 2 = buy\n",
        "        # e.g. [2,1,0] means:\n",
        "        # buy first stock\n",
        "        # hold second stock\n",
        "        # sell third stock\n",
        "        action_vec = self.action_list[action]\n",
        "\n",
        "        # determine which stocks to buy or sell\n",
        "        sell_index = []  # stores index of stocks we want to sell\n",
        "        buy_index = []  # stores index of stocks we want to buy\n",
        "        for i, a in enumerate(action_vec):\n",
        "            if a == 0:\n",
        "                sell_index.append(i)\n",
        "            elif a == 2:\n",
        "                buy_index.append(i)\n",
        "\n",
        "        # sell any stocks we want to sell\n",
        "        # then buy any stocks we want to buy\n",
        "        if sell_index:\n",
        "            # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
        "            for i in sell_index:\n",
        "                self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "                self.stock_owned[i] = 0\n",
        "        if buy_index:\n",
        "            # NOTE: when buying, we will loop through each stock we want to buy,\n",
        "            #       and buy one share at a time until we run out of cash\n",
        "            can_buy = True\n",
        "            while can_buy:\n",
        "                for i in buy_index:\n",
        "                    if self.cash_in_hand > self.stock_price[i]:\n",
        "                        self.stock_owned[i] += 1  # buy one share\n",
        "                        self.cash_in_hand -= self.stock_price[i]\n",
        "                    else:\n",
        "                        can_buy = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AqUa7OJCS2bH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tf_agents.trajectories import trajectory\n",
        "\n",
        "def get_data():\n",
        "    # returns a T x 3 list of stock prices\n",
        "    # each row is a different stock\n",
        "    # 0 = AAPL\n",
        "    # 1 = MSI\n",
        "    # 2 = SBUX\n",
        "    df = pd.read_csv('/content/drive/My Drive/ReinforcementLearning/stockprices.csv')\n",
        "    return df.values\n",
        "\n",
        "\n",
        "def get_scaler(env):\n",
        "    # return scikit-learn scaler object to scale the states\n",
        "    # Note: you could also populate the replay buffer here\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.n_step):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        timestep = env._step(action)\n",
        "        states.append(timestep[3])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "\n",
        "        time_step = environment._reset()\n",
        "        episode_return = 0.0\n",
        "        step_cnt = 0\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = environment._step(action_step.action)\n",
        "            episode_return += time_step.reward\n",
        "            step_cnt = step_cnt + 1\n",
        "            total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "def collect_step(environment, policy):\n",
        "    time_step = environment.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = environment.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    print(traj)\n",
        "    # Add trajectory to the replay buffer\n",
        "    #replay_buffer.add_batch(traj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nMHSOcYgS67g"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#from AgentUtils import get_data, compute_avg_return\n",
        "#from StockTraderEnv import StockTraderEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfWGOZQyTOhg",
        "outputId": "bb3164c3-6349-4e93-90ec-debb43dbfff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eow8qU7JS_GM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e36042a-850b-40fd-8b1a-26bd9a69ce03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 67.8542,  60.3   ,  28.185 ],\n",
              "       [ 68.5614,  60.9   ,  28.07  ],\n",
              "       [ 66.8428,  60.83  ,  28.13  ],\n",
              "       ...,\n",
              "       [156.49  , 101.06  ,  54.69  ],\n",
              "       [163.03  , 102.76  ,  55.61  ],\n",
              "       [159.54  , 102.63  ,  54.46  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = get_data()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E2MWUcD-yFcm"
      },
      "outputs": [],
      "source": [
        "scaler_env = StockTraderEnv(data)\n",
        "scaler_env._reset()\n",
        "scaler = get_scaler(scaler_env)\n",
        "scaler_env.set_scaler(scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_35gRvaCTFJL"
      },
      "outputs": [],
      "source": [
        "train_qt_env = StockTraderEnv(data)\n",
        "train_qt_env._reset()\n",
        "train_qt_env.set_scaler(scaler)\n",
        "train_qt_env._reset()\n",
        "from tf_agents.environments import utils\n",
        "utils.validate_py_environment(train_qt_env, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PvsC-S27Ol0A"
      },
      "outputs": [],
      "source": [
        "eval_qt_env = StockTraderEnv(data)\n",
        "eval_qt_env._reset()\n",
        "eval_qt_env.set_scaler(scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glVueyYurhBW",
        "outputId": "0257aeeb-16d6-4e11-cd32-0fb70cd5fdf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': array(1., dtype=float32),\n",
              " 'observation': array([-0.73307339, -0.67589427, -0.58934718, -1.34735989, -0.94341068,\n",
              "       -1.93681045,  1.34768666]),\n",
              " 'reward': array(0., dtype=float32),\n",
              " 'step_type': array(0, dtype=int32)})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_qt_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISuAo4r7pMIu",
        "outputId": "781f1ef0-cfdc-4ae8-d60e-b1062abc32b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([-0.73307339, -0.67589427,  2.32476201, -1.34735989, -0.94341068,\n",
            "       -1.93681045, -0.45700713]),\n",
            " 'reward': array(-6.91216e-11, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n"
          ]
        }
      ],
      "source": [
        "o1 = eval_qt_env._reset()\n",
        "total_reward = 0.0\n",
        "while True:\n",
        "    #action = eval_qt_env.action_space.sample()\n",
        "    action = 2\n",
        "    ts_1 = eval_qt_env._step(action)\n",
        "    print(ts_1)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ggwvdDxp8F",
        "outputId": "550963e7-9d46-4f60-a583-59b2098a81a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
              " 'observation': BoundedArraySpec(shape=(7,), dtype=dtype('float64'), name='observation', minimum=-1.7976931348623157e+308, maximum=1.7976931348623157e+308),\n",
              " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
              " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "train_qt_env.time_step_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ucjJq7kppL6n"
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "#train_py_env = suite_gym.wrap_env(train_qt_env)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_qt_env)\n",
        "\n",
        "#eval_py_env = suite_gym.wrap_env(eval_qt_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_qt_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G02BqJVPpLp9",
        "outputId": "bf04799a-d241-4c9d-a83d-b211c2d06f99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(26, dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_env.action_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6SPM__Bs2XU",
        "outputId": "6a8798dd-7592-469d-ef77-bb52bd22c6f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'observation': BoundedTensorSpec(shape=(7,), dtype=tf.float64, name='observation', minimum=array(-1.79769313e+308), maximum=array(1.79769313e+308)),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_env.time_step_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-8Us6tCjs2NJ"
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies import random_tf_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HQ_FVCbes2AT"
      },
      "outputs": [],
      "source": [
        "time_step = train_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_4aqrdhtKEm",
        "outputId": "c72fdaf8-73e5-4ca3-f512-46c3739ea613"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(26, dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "random_policy.action_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tShQMJgstJ3n"
      },
      "outputs": [],
      "source": [
        "action_step = random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar2Y-vyytJns",
        "outputId": "24efcad8-e895-4384-9b4c-640d0606dc96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "action_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HmJCOyvxbDdi"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3 \n",
        "num_eval_episodes = 10\n",
        "replay_buffer_max_length = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rszgpm2dbDPT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.networks import sequential\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WWVx2LIKbDBv"
      },
      "outputs": [],
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5VdWSHm-Ub_j"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7qTvzKLqbTQO"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "320hglLFbVCS"
      },
      "outputs": [],
      "source": [
        "time_step = train_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tia1kmb9bYTT",
        "outputId": "6d366403-9fb1-4b41-e29a-3735d961635a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "u8oMcpnfbaF5"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=5):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment._reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment._step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjxZRuvbjbC",
        "outputId": "64df9f81-a427-4c47-9779-e1f29e6f233d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21851.824"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "compute_avg_return(eval_env, random_policy, num_episodes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tl1S9Xn8bwCU"
      },
      "outputs": [],
      "source": [
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KJ2-SP8hbl2i"
      },
      "outputs": [],
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nfSw35Gabpjc"
      },
      "outputs": [],
      "source": [
        "train_env._reset()\n",
        "\n",
        "init_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    random_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=2_500)\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-OQPY9mcDAV",
        "outputId": "9e32b2a3-d02b-4fc0-ba9f-1433d199e5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-38750cf20376>:1: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[16, 22, 16, ..., 12, 20, 18]], dtype=int32)>,\n",
              " 'discount': <tf.Tensor: shape=(1, 2501), dtype=float32, numpy=array([[1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>,\n",
              " 'next_step_type': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>,\n",
              " 'observation': <tf.Tensor: shape=(1, 2501, 7), dtype=float64, numpy=\n",
              "array([[[-0.64654901, -0.71084845, -0.62350721, ..., -0.94341068,\n",
              "         -1.93681045,  1.54235472],\n",
              "        [-0.64654901,  1.27080322, -0.62350721, ..., -0.94341068,\n",
              "         -1.93681045, -0.45487248],\n",
              "        [-0.64654901,  1.27080322, -0.62350721, ..., -0.88720107,\n",
              "         -1.94786619, -0.45487248],\n",
              "        ...,\n",
              "        [-0.64654901,  2.09699032, -0.61927917, ...,  2.03757235,\n",
              "          1.04295324, -0.45724993],\n",
              "        [-0.64654901,  2.09699032, -0.62350721, ...,  2.06192985,\n",
              "          1.10448086, -0.45126404],\n",
              "        [ 1.07367264, -0.71084845,  0.16290899, ...,  2.17153859,\n",
              "          1.12178551, -0.4575091 ]]])>,\n",
              " 'policy_info': (),\n",
              " 'reward': <tf.Tensor: shape=(1, 2501), dtype=float32, numpy=\n",
              "array([[ 1.2369128e-10,  1.9860001e+02, -2.3170000e+01, ...,\n",
              "         1.2258000e+02,  5.4872998e+02,  4.1287000e+02]], dtype=float32)>,\n",
              " 'step_type': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[0, 1, 1, ..., 1, 1, 1]], dtype=int32)>})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "replay_buffer.gather_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93cprqXucXu0",
        "outputId": "36089335-7224-45b3-e120-48d49db5502a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-34-508fce8195e4>:1: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "trajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RZVBtdgfco16"
      },
      "outputs": [],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2).prefetch(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtTVSpMcc0_M",
        "outputId": "972de3db-8608-4c57-9eed-8f65b751e26b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(64, 2, 7), dtype=tf.float64, name=None),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(64,), dtype=tf.float32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0PMItBvZc3sl"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cBebrKG5dA6X"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100_000   # less intelligence, more persistance; 24x7 player\n",
        "save_interval = 50_000\n",
        "eval_interval = 5_000\n",
        "log_interval = 5_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "exnGcosQdE36"
      },
      "outputs": [],
      "source": [
        "# Create a driver to collect experience.\n",
        "collect_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=4) # collect 4 steps for each training iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YeGuD1GcdGqC"
      },
      "outputs": [],
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "collect_driver.run = common.function(collect_driver.run)\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(train_env, agent.policy, num_episodes=1)\n",
        "returns = np.array([avg_return])\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = None\n",
        "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RqUTYoVdPI0",
        "outputId": "b7b781a2-c38d-4616-8e78-d9097d29fbaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4562.3535], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XSfMTrzFdYld",
        "outputId": "b2f2d588-b770-45aa-88c8-5ffb63cc8a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " step 105000step = 105000: loss = 7.854769506709668e+17\n",
            "step = 105000: Average Return = 19444.35546875\n",
            " step 110000step = 110000: loss = 7.250478190961885e+18\n",
            "step = 110000: Average Return = 19444.35546875\n",
            " step 115000step = 115000: loss = 9.13726543299294e+18\n",
            "step = 115000: Average Return = 19444.35546875\n",
            " step 120000step = 120000: loss = 8.819821582647034e+18\n",
            "step = 120000: Average Return = 19444.35546875\n",
            " step 125000step = 125000: loss = 9.803136821599666e+18\n",
            "step = 125000: Average Return = 19444.35546875\n",
            " step 130000step = 130000: loss = 4.080930864978985e+18\n",
            "step = 130000: Average Return = 19444.35546875\n",
            " step 135000step = 135000: loss = 3.0860516224323813e+19\n",
            "step = 135000: Average Return = 19444.35546875\n",
            " step 140000step = 140000: loss = 2.0447349460913095e+19\n",
            "step = 140000: Average Return = 19444.35546875\n",
            " step 145000step = 145000: loss = 4.1451086989852934e+19\n",
            "step = 145000: Average Return = 19444.35546875\n",
            " step 150000step = 150000: loss = 3.9028220759071785e+19\n",
            "step = 150000: Average Return = 19444.35546875\n",
            " step 155000step = 155000: loss = 5.916220500802221e+19\n",
            "step = 155000: Average Return = 19444.35546875\n",
            " step 160000step = 160000: loss = 8.746772449023178e+19\n",
            "step = 160000: Average Return = 19444.35546875\n",
            " step 165000step = 165000: loss = 1.5328769890383299e+19\n",
            "step = 165000: Average Return = 19444.35546875\n",
            " step 170000step = 170000: loss = 6.8799450813757915e+19\n",
            "step = 170000: Average Return = 19444.35546875\n",
            " step 175000step = 175000: loss = 1.2573522394037092e+20\n",
            "step = 175000: Average Return = 19444.35546875\n",
            " step 180000step = 180000: loss = 3.481101312441136e+19\n",
            "step = 180000: Average Return = 19444.35546875\n",
            " step 185000step = 185000: loss = 1.8699557860916645e+20\n",
            "step = 185000: Average Return = 19444.35546875\n",
            " step 190000step = 190000: loss = 1.6629791433107387e+20\n",
            "step = 190000: Average Return = 19444.35546875\n",
            " step 195000step = 195000: loss = 1.3304385365205844e+20\n",
            "step = 195000: Average Return = 19444.35546875\n",
            " step 200000step = 200000: loss = 1.768341032514932e+20\n",
            "step = 200000: Average Return = 19444.35546875\n",
            " step 205000step = 205000: loss = 1.0587786802087592e+20\n",
            "step = 205000: Average Return = 19444.35546875\n",
            " step 210000step = 210000: loss = 2.0124090403519044e+20\n",
            "step = 210000: Average Return = 19444.35546875\n",
            " step 215000step = 215000: loss = 3.707068368213288e+20\n",
            "step = 215000: Average Return = 19444.35546875\n",
            " step 220000step = 220000: loss = 3.7634224733005283e+20\n",
            "step = 220000: Average Return = 19444.35546875\n",
            " step 225000step = 225000: loss = 3.317137916382528e+20\n",
            "step = 225000: Average Return = 19444.35546875\n",
            " step 230000step = 230000: loss = 5.021586416168327e+20\n",
            "step = 230000: Average Return = 19444.35546875\n",
            " step 235000step = 235000: loss = 5.5933208517690576e+20\n",
            "step = 235000: Average Return = 19444.35546875\n",
            " step 240000step = 240000: loss = 5.0638643095139883e+20\n",
            "step = 240000: Average Return = 19444.35546875\n",
            " step 245000step = 245000: loss = 4.6075754282362497e+20\n",
            "step = 245000: Average Return = 19444.35546875\n",
            " step 250000step = 250000: loss = 9.073289579272654e+20\n",
            "step = 250000: Average Return = 19444.35546875\n",
            " step 255000step = 255000: loss = 1.4189100799426036e+20\n",
            "step = 255000: Average Return = 19444.35546875\n",
            " step 260000step = 260000: loss = 3.8427502141431716e+20\n",
            "step = 260000: Average Return = 19444.35546875\n",
            " step 265000step = 265000: loss = 1.1898974649224423e+21\n",
            "step = 265000: Average Return = 19444.35546875\n",
            " step 270000step = 270000: loss = 1.2114383226776437e+21\n",
            "step = 270000: Average Return = 19444.35546875\n",
            " step 275000step = 275000: loss = 1.355739002763152e+21\n",
            "step = 275000: Average Return = 19444.35546875\n",
            " step 280000step = 280000: loss = 2.345527603230176e+21\n",
            "step = 280000: Average Return = 19444.35546875\n",
            " step 285000step = 285000: loss = 8.762658017099425e+20\n",
            "step = 285000: Average Return = 19444.35546875\n",
            " step 290000step = 290000: loss = 1.6586447805131155e+21\n",
            "step = 290000: Average Return = 19444.35546875\n",
            " step 295000step = 295000: loss = 1.5418284430535426e+21\n",
            "step = 295000: Average Return = 19444.35546875\n",
            " step 300000step = 300000: loss = 1.8668739165716595e+21\n",
            "step = 300000: Average Return = 19444.35546875\n",
            " step 305000step = 305000: loss = 1.8180616520104044e+21\n",
            "step = 305000: Average Return = 19444.35546875\n",
            " step 310000step = 310000: loss = 3.291737825590391e+21\n",
            "step = 310000: Average Return = 19444.35546875\n",
            " step 315000step = 315000: loss = 3.5308429370067455e+21\n",
            "step = 315000: Average Return = 19444.35546875\n",
            " step 320000step = 320000: loss = 2.7881287082341963e+21\n",
            "step = 320000: Average Return = 19444.35546875\n",
            " step 325000step = 325000: loss = 2.449992255761732e+21\n",
            "step = 325000: Average Return = 19444.35546875\n",
            " step 330000step = 330000: loss = 1.4299815463077963e+21\n",
            "step = 330000: Average Return = 19444.35546875\n",
            " step 335000step = 335000: loss = 2.8111801014519155e+21\n",
            "step = 335000: Average Return = 19444.35546875\n",
            " step 340000step = 340000: loss = 7.944973491229946e+21\n",
            "step = 340000: Average Return = 19444.35546875\n",
            " step 345000step = 345000: loss = 2.579092161204958e+21\n",
            "step = 345000: Average Return = 19444.35546875\n",
            " step 350000step = 350000: loss = 5.994912037828754e+21\n",
            "step = 350000: Average Return = 19444.35546875\n",
            " step 355000step = 355000: loss = 3.9493382435302885e+21\n",
            "step = 355000: Average Return = 19444.35546875\n",
            " step 360000step = 360000: loss = 5.410540149829901e+21\n",
            "step = 360000: Average Return = 19444.35546875\n",
            " step 365000step = 365000: loss = 9.227608235254271e+21\n",
            "step = 365000: Average Return = 19444.35546875\n",
            " step 370000step = 370000: loss = 5.582779191678778e+21\n",
            "step = 370000: Average Return = 19444.35546875\n",
            " step 375000step = 375000: loss = 7.25643784744964e+21\n",
            "step = 375000: Average Return = 19444.35546875\n",
            " step 380000step = 380000: loss = 8.576547606923269e+21\n",
            "step = 380000: Average Return = 19444.35546875\n",
            " step 385000step = 385000: loss = 9.346153110545824e+21\n",
            "step = 385000: Average Return = 19444.35546875\n",
            " step 390000step = 390000: loss = 4.148441538631408e+21\n",
            "step = 390000: Average Return = 19444.35546875\n",
            " step 395000step = 395000: loss = 1.3283985219978135e+22\n",
            "step = 395000: Average Return = 19444.35546875\n",
            " step 400000step = 400000: loss = 1.4063124844011833e+22\n",
            "step = 400000: Average Return = 19444.35546875\n",
            " step 405000step = 405000: loss = 1.5354483904939305e+21\n",
            "step = 405000: Average Return = 19444.35546875\n",
            " step 410000step = 410000: loss = 7.704499285526871e+21\n",
            "step = 410000: Average Return = 19444.35546875\n",
            " step 415000step = 415000: loss = 1.7208170859589559e+22\n",
            "step = 415000: Average Return = 19444.35546875\n",
            " step 420000step = 420000: loss = 9.051943079988871e+21\n",
            "step = 420000: Average Return = 19444.35546875\n",
            " step 425000step = 425000: loss = 2.124693145142101e+22\n",
            "step = 425000: Average Return = 19444.35546875\n",
            " step 430000step = 430000: loss = 1.5573332106706724e+21\n",
            "step = 430000: Average Return = 19444.35546875\n",
            " step 435000step = 435000: loss = 6.550528442206846e+21\n",
            "step = 435000: Average Return = 19444.35546875\n",
            " step 440000step = 440000: loss = 9.959895223514574e+21\n",
            "step = 440000: Average Return = 19444.35546875\n",
            " step 445000step = 445000: loss = 1.290463914206537e+22\n",
            "step = 445000: Average Return = 19444.35546875\n",
            " step 450000step = 450000: loss = 1.06930170665558e+22\n",
            "step = 450000: Average Return = 19444.35546875\n",
            " step 455000step = 455000: loss = 3.0563021345604275e+22\n",
            "step = 455000: Average Return = 19444.35546875\n",
            " step 460000step = 460000: loss = 3.4547918885892374e+22\n",
            "step = 460000: Average Return = 19444.35546875\n",
            " step 465000step = 465000: loss = 4.339793801003851e+22\n",
            "step = 465000: Average Return = 19444.35546875\n",
            " step 470000step = 470000: loss = 1.1322103506404955e+22\n",
            "step = 470000: Average Return = 19444.35546875\n",
            " step 475000step = 475000: loss = 1.97080987465446e+22\n",
            "step = 475000: Average Return = 19444.35546875\n",
            " step 480000step = 480000: loss = 3.795247807459788e+22\n",
            "step = 480000: Average Return = 19444.35546875\n",
            " step 485000step = 485000: loss = 6.832483052277817e+22\n",
            "step = 485000: Average Return = 19444.35546875\n",
            " step 490000step = 490000: loss = 3.856272933490547e+22\n",
            "step = 490000: Average Return = 19444.35546875\n",
            " step 495000step = 495000: loss = 4.613667151903468e+22\n",
            "step = 495000: Average Return = 19444.35546875\n",
            " step 500000step = 500000: loss = 4.407286546459476e+22\n",
            "step = 500000: Average Return = 19444.35546875\n",
            " step 505000step = 505000: loss = 5.011676822212e+22\n",
            "step = 505000: Average Return = 19444.35546875\n",
            " step 510000step = 510000: loss = 2.763748753126403e+21\n",
            "step = 510000: Average Return = 19444.35546875\n",
            " step 515000step = 515000: loss = 2.688561607707415e+22\n",
            "step = 515000: Average Return = 19444.35546875\n",
            " step 520000step = 520000: loss = 7.92282343718265e+22\n",
            "step = 520000: Average Return = 19444.35546875\n",
            " step 525000step = 525000: loss = 2.4903122513906404e+22\n",
            "step = 525000: Average Return = 19444.35546875\n",
            " step 530000step = 530000: loss = 5.897167825546146e+22\n",
            "step = 530000: Average Return = 19444.35546875\n",
            " step 535000step = 535000: loss = 5.84770208867896e+22\n",
            "step = 535000: Average Return = 19444.35546875\n",
            " step 540000step = 540000: loss = 1.7227534086187438e+22\n",
            "step = 540000: Average Return = 19444.35546875\n",
            " step 545000step = 545000: loss = 1.0217253424220661e+23\n",
            "step = 545000: Average Return = 19444.35546875\n",
            " step 550000step = 550000: loss = 3.408984425699364e+22\n",
            "step = 550000: Average Return = 19444.35546875\n",
            " step 555000step = 555000: loss = 9.124919746120755e+22\n",
            "step = 555000: Average Return = 19444.35546875\n",
            " step 556163"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-fc9c702c5656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-42-1d434327da53>\", line 8, in <module>\n      train_loss = agent.train(experience).loss\n    File \"/usr/local/lib/python3.7/dist-packages/tf_agents/agents/tf_agent.py\", line 331, in train\n      experience=experience, weights=weights, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tf_agents/agents/dqn/dqn_agent.py\", line 395, in _train\n      tf.debugging.check_numerics(loss_info.loss, 'Loss is inf or nan')\nNode: 'CheckNumerics'\nLoss is inf or nan : Tensor had NaN values\n\t [[{{node CheckNumerics}}]] [Op:__inference_train_221241]"
          ]
        }
      ],
      "source": [
        "num_iterations = 1_000_000\n",
        "while True:\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "    print(f'\\r step {step}', end='')\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        avg_return = compute_avg_return(train_env, agent.policy, num_episodes=1)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "        returns = np.append(returns, avg_return)\n",
        "\n",
        "    # if step % save_interval == 0:\n",
        "    #     save_checkpoint_to_local()\n",
        "\n",
        "    if step > num_iterations:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-osKaDQdclo"
      },
      "outputs": [],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M4vR0beHJuS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TFAgents-StockTrader.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}