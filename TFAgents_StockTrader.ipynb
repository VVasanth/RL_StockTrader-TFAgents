{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VVasanth/RL_StockTrader-TFAgents/blob/main/TFAgents_StockTrader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48L2GKE9Yuof",
        "outputId": "c7cbe079-f232-41d9-a0ea-7e11c9eddba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym>=0.21.0\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[K     |████████████████████████████████| 626 kB 4.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (4.11.3)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.21.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.21.0) (4.2.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701375 sha256=8c39386103edb5627803fe6b78e26daa56e6c5039f5f3af62be2db9caee3e0c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/33/04/6723848e46f0f1ebe794bb329b7c761c3329a0d7ffade99da7\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.23.1 gym-notices-0.0.6\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.2.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.23.1)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 94 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (0.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.17.0->tf-agents) (3.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Installing collected packages: pygame, tf-agents\n",
            "Successfully installed pygame-2.1.0 tf-agents-0.12.1\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,575 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Err:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [84.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [909 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,167 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,733 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,496 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [942 kB]\n",
            "Reading package lists... Done\n",
            "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
            "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
            "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 0s (2,461 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155514 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting imageio==2.4.0\n",
            "  Downloading imageio-2.4.0.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-py3-none-any.whl size=3303895 sha256=803186b6166213b23434980fd36ec84e538a347acbafc2ee5ab2926429971adb\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/de/2f/6c5a75120d68a2c3138120c8d0ce1c6f9483a4b96307986bf2\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed imageio-2.4.0\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.23.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.1.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.0)\n",
            "Collecting dm-reverb~=0.7.0\n",
            "  Downloading dm_reverb-0.7.2-cp37-cp37m-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (0.1.7)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (0.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.17.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (13.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.24.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n",
            "Installing collected packages: tf-estimator-nightly, dm-reverb\n",
            "Successfully installed dm-reverb-0.7.2 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gym>=0.21.0\"\n",
        "!pip install tf-agents\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9INYihrmlmWv"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import itertools\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import numpy as np\n",
        "\n",
        "class StockTraderEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, data, initInvestment=20000):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=26, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(7,), dtype=np.float64, name='observation')\n",
        "        self.stock_price_history = data\n",
        "        self.init_investment = initInvestment\n",
        "        self.state = 0\n",
        "        self.episode_ended = False\n",
        "        self.stock_owned = None\n",
        "        self.stock_price = None\n",
        "        self.cash_in_hand = None\n",
        "        self.n_stock = 3\n",
        "        self.n_step = self.stock_price_history.shape[0]\n",
        "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
        "#[[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 1, 0], [0, 1, 1], [0, 1, 2], [0, 2, 0], [0, 2, 1], [0, 2, 2], [1, 0, 0], [1, 0, 1], [1, 0, 2], [1, 1, 0],\n",
        "#[1, 1, 1], [1, 1, 2], [1, 2, 0], [1, 2, 1], [1, 2, 2], [2, 0, 0], [2, 0, 1], [2, 0, 2], [2, 1, 0], [2, 1, 1], [2, 1, 2], [2, 2, 0], [2, 2, 1], [2, 2, 2]]\n",
        "        self.action_space = np.arange(3 ** self.n_stock)\n",
        "        ###[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]##np.ndarray\n",
        "        self.scaler = None\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def set_scaler(self, scaler):\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def get_obs(self):\n",
        "        obs = np.empty(7)\n",
        "        obs[:self.n_stock] = self.stock_owned\n",
        "        obs[self.n_stock:2 * self.n_stock] = self.stock_price\n",
        "        obs[-1] = self.cash_in_hand\n",
        "        if self.scaler != None:\n",
        "            obs = self.scaler.transform([obs]).flatten()\n",
        "        return obs\n",
        "\n",
        "    def get_val(self):\n",
        "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = 0\n",
        "        self.episode_ended = False\n",
        "        self.stock_owned = np.zeros(self.n_stock)\n",
        "        self.stock_price = self.stock_price_history[self.state]\n",
        "        self.cash_in_hand = self.init_investment\n",
        "        return ts.restart(self.get_obs())\n",
        "\n",
        "    def _step(self, action):\n",
        "\n",
        "        if self.episode_ended:\n",
        "            # The last action ended the episode. Ignore the current action and start\n",
        "            # a new episode.\n",
        "            return self._reset()\n",
        "\n",
        "        # get current value before performing the action\n",
        "        prev_val = self.get_val()\n",
        "\n",
        "        self.stock_price = self.stock_price_history[self.state]\n",
        "\n",
        "        # update price, i.e. go to the next day\n",
        "        self.state += 1\n",
        "\n",
        "        # perform the trade\n",
        "        self.trade(action)\n",
        "\n",
        "        # get the new value after taking the action\n",
        "        cur_val = self.get_val()\n",
        "\n",
        "        # reward is the increase in porfolio value\n",
        "        reward = cur_val - prev_val\n",
        "\n",
        "        # done if we have run out of data\n",
        "        done = self.state == self.n_step - 1\n",
        "\n",
        "        # store the current value of the portfolio here\n",
        "        info = {'cur_val': self.state}\n",
        "\n",
        "        # conform to the Gym API\n",
        "        # return self._get_obs(), reward, done, info\n",
        "        if (done != True):\n",
        "            return ts.transition(self.get_obs(), reward=reward, discount=1.0)\n",
        "        else:\n",
        "            self.episode_ended = True\n",
        "            return ts.termination(self.get_obs(), reward)\n",
        "\n",
        "    def trade(self, action):\n",
        "        # index the action we want to perform\n",
        "        # 0 = sell\n",
        "        # 1 = hold\n",
        "        # 2 = buy\n",
        "        # e.g. [2,1,0] means:\n",
        "        # buy first stock\n",
        "        # hold second stock\n",
        "        # sell third stock\n",
        "        action_vec = self.action_list[action]\n",
        "\n",
        "        # determine which stocks to buy or sell\n",
        "        sell_index = []  # stores index of stocks we want to sell\n",
        "        buy_index = []  # stores index of stocks we want to buy\n",
        "        for i, a in enumerate(action_vec):\n",
        "            if a == 0:\n",
        "                sell_index.append(i)\n",
        "            elif a == 2:\n",
        "                buy_index.append(i)\n",
        "\n",
        "        # sell any stocks we want to sell\n",
        "        # then buy any stocks we want to buy\n",
        "        if sell_index:\n",
        "            # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
        "            for i in sell_index:\n",
        "                self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "                self.stock_owned[i] = 0\n",
        "        if buy_index:\n",
        "            # NOTE: when buying, we will loop through each stock we want to buy,\n",
        "            #       and buy one share at a time until we run out of cash\n",
        "            can_buy = True\n",
        "            while can_buy:\n",
        "                for i in buy_index:\n",
        "                    if self.cash_in_hand > self.stock_price[i]:\n",
        "                        self.stock_owned[i] += 1  # buy one share\n",
        "                        self.cash_in_hand -= self.stock_price[i]\n",
        "                    else:\n",
        "                        can_buy = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AqUa7OJCS2bH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tf_agents.trajectories import trajectory\n",
        "\n",
        "def get_data():\n",
        "    # returns a T x 3 list of stock prices\n",
        "    # each row is a different stock\n",
        "    # 0 = AAPL\n",
        "    # 1 = MSI\n",
        "    # 2 = SBUX\n",
        "    df = pd.read_csv('/content/drive/My Drive/ReinforcementLearning/stockprices.csv')\n",
        "    return df.values\n",
        "\n",
        "\n",
        "def get_scaler(env):\n",
        "    # return scikit-learn scaler object to scale the states\n",
        "    # Note: you could also populate the replay buffer here\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.n_step):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        timestep = env._step(action)\n",
        "        states.append(timestep[3])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler\n",
        "\n",
        "\n",
        "def collect_step(environment, policy):\n",
        "    time_step = environment.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = environment.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    print(traj)\n",
        "    # Add trajectory to the replay buffer\n",
        "    #replay_buffer.add_batch(traj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfWGOZQyTOhg",
        "outputId": "33075389-55fd-4b08-8db6-aaeeb07aaaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eow8qU7JS_GM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad383ed4-aa6e-4e77-b02e-9773b04bac60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 67.8542,  60.3   ,  28.185 ],\n",
              "       [ 68.5614,  60.9   ,  28.07  ],\n",
              "       [ 66.8428,  60.83  ,  28.13  ],\n",
              "       ...,\n",
              "       [156.49  , 101.06  ,  54.69  ],\n",
              "       [163.03  , 102.76  ,  55.61  ],\n",
              "       [159.54  , 102.63  ,  54.46  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = get_data()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E2MWUcD-yFcm"
      },
      "outputs": [],
      "source": [
        "scaler_env = StockTraderEnv(data)\n",
        "scaler_env._reset()\n",
        "scaler = get_scaler(scaler_env)\n",
        "scaler_env.set_scaler(scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_35gRvaCTFJL"
      },
      "outputs": [],
      "source": [
        "train_qt_env = StockTraderEnv(data)\n",
        "train_qt_env._reset()\n",
        "train_qt_env.set_scaler(scaler)\n",
        "train_qt_env._reset()\n",
        "from tf_agents.environments import utils\n",
        "utils.validate_py_environment(train_qt_env, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PvsC-S27Ol0A"
      },
      "outputs": [],
      "source": [
        "eval_qt_env = StockTraderEnv(data)\n",
        "eval_qt_env._reset()\n",
        "eval_qt_env.set_scaler(scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glVueyYurhBW",
        "outputId": "a9fa648f-0ef2-42ef-deb7-84cb828e318d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': array(1., dtype=float32),\n",
              " 'observation': array([-0.65187811, -0.64139681, -0.66663252, -1.34735989, -0.94341068,\n",
              "       -1.93681045,  1.81764853]),\n",
              " 'reward': array(0., dtype=float32),\n",
              " 'step_type': array(0, dtype=int32)})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_qt_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISuAo4r7pMIu",
        "outputId": "5c12f6b8-a756-4a3a-b0f1-20f1cf36a392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([-0.65187811, -0.64139681,  2.49511586, -1.34735989, -0.94341068,\n",
            "       -1.93681045, -0.45608382]),\n",
            " 'reward': array(-6.91216e-11, dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n"
          ]
        }
      ],
      "source": [
        "o1 = eval_qt_env._reset()\n",
        "total_reward = 0.0\n",
        "while True:\n",
        "    #action = eval_qt_env.action_space.sample()\n",
        "    action = 2\n",
        "    ts_1 = eval_qt_env._step(action)\n",
        "    print(ts_1)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ggwvdDxp8F",
        "outputId": "958186e7-03ba-4e49-e212-4f7f2c38c20c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
              " 'observation': BoundedArraySpec(shape=(7,), dtype=dtype('float64'), name='observation', minimum=-1.7976931348623157e+308, maximum=1.7976931348623157e+308),\n",
              " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
              " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "train_qt_env.time_step_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ucjJq7kppL6n"
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "#train_py_env = suite_gym.wrap_env(train_qt_env)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_qt_env)\n",
        "\n",
        "#eval_py_env = suite_gym.wrap_env(eval_qt_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_qt_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G02BqJVPpLp9",
        "outputId": "dfff6305-ba30-4fea-bf63-b2c59d34b76a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(26, dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_env.action_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6SPM__Bs2XU",
        "outputId": "6b9d3657-b614-4706-fa55-1dfe5d7e9fb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'observation': BoundedTensorSpec(shape=(7,), dtype=tf.float64, name='observation', minimum=array(-1.79769313e+308), maximum=array(1.79769313e+308)),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_env.time_step_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-8Us6tCjs2NJ"
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies import random_tf_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HQ_FVCbes2AT"
      },
      "outputs": [],
      "source": [
        "time_step = train_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_4aqrdhtKEm",
        "outputId": "ea265048-2c9f-433a-e9f3-0a2f2e9b33dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(26, dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "random_policy.action_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tShQMJgstJ3n"
      },
      "outputs": [],
      "source": [
        "action_step = random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar2Y-vyytJns",
        "outputId": "e5f69988-22e2-4b64-818b-4378fca50b30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "action_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HmJCOyvxbDdi"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3 \n",
        "num_eval_episodes = 10\n",
        "replay_buffer_max_length = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rszgpm2dbDPT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.networks import sequential\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WWVx2LIKbDBv"
      },
      "outputs": [],
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5VdWSHm-Ub_j"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7qTvzKLqbTQO"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "320hglLFbVCS"
      },
      "outputs": [],
      "source": [
        "time_step = train_env._reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tia1kmb9bYTT",
        "outputId": "da1a7259-f2ce-431c-9ed5-7f2045307e64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "u8oMcpnfbaF5"
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=5):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment._reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment._step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjxZRuvbjbC",
        "outputId": "dfa47abd-eb2d-4e75-96a6-061728e7cbec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7861.42"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "compute_avg_return(eval_env, random_policy, num_episodes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tl1S9Xn8bwCU"
      },
      "outputs": [],
      "source": [
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KJ2-SP8hbl2i"
      },
      "outputs": [],
      "source": [
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nfSw35Gabpjc"
      },
      "outputs": [],
      "source": [
        "train_env._reset()\n",
        "\n",
        "init_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    random_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=2_500)\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-OQPY9mcDAV",
        "outputId": "7b3a58fb-3539-46f0-e319-d04191bd76a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-34-38750cf20376>:1: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[23,  4, 18, ..., 10, 24,  8]], dtype=int32)>,\n",
              " 'discount': <tf.Tensor: shape=(1, 2501), dtype=float32, numpy=array([[1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>,\n",
              " 'next_step_type': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>,\n",
              " 'observation': <tf.Tensor: shape=(1, 2501, 7), dtype=float64, numpy=\n",
              "array([[[-0.65187811, -0.64139681, -0.66663252, ..., -0.94341068,\n",
              "         -1.93681045,  1.81764853],\n",
              "        [ 1.47538518, -0.64139681,  0.26093259, ..., -0.94341068,\n",
              "         -1.93681045, -0.45528605],\n",
              "        [-0.65187811, -0.64139681,  0.26093259, ..., -0.88720107,\n",
              "         -1.94786619,  1.16733864],\n",
              "        ...,\n",
              "        [-0.65187811, -0.64139681,  1.24647052, ...,  2.03757235,\n",
              "          1.04295324, -0.45276851],\n",
              "        [-0.65187811, -0.64139681,  1.24647052, ...,  2.06192985,\n",
              "          1.10448086, -0.45276851],\n",
              "        [ 0.32993572,  0.03156486, -0.66663252, ...,  2.17153859,\n",
              "          1.12178551, -0.44982496]]])>,\n",
              " 'policy_info': (),\n",
              " 'reward': <tf.Tensor: shape=(1, 2501), dtype=float32, numpy=\n",
              "array([[-5.0931703e-11,  1.2317760e+02,  1.2480000e+01, ...,\n",
              "         2.7456000e+02,  7.7220001e+01,  4.6066000e+02]], dtype=float32)>,\n",
              " 'step_type': <tf.Tensor: shape=(1, 2501), dtype=int32, numpy=array([[0, 1, 1, ..., 1, 1, 1]], dtype=int32)>})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "replay_buffer.gather_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93cprqXucXu0",
        "outputId": "0d87f279-f6cc-40a3-dee9-b57ab76aace8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-35-508fce8195e4>:1: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "trajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RZVBtdgfco16"
      },
      "outputs": [],
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2).prefetch(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtTVSpMcc0_M",
        "outputId": "8f76da6b-0c57-4fca-e4c8-d965aa161ac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(64, 2, 7), dtype=tf.float64, name=None),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(64,), dtype=tf.float32, name=None)))>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0PMItBvZc3sl"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cBebrKG5dA6X"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100_000   # less intelligence, more persistance; 24x7 player\n",
        "save_interval = 50_000\n",
        "eval_interval = 5_000\n",
        "log_interval = 5_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "exnGcosQdE36"
      },
      "outputs": [],
      "source": [
        "# Create a driver to collect experience.\n",
        "collect_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=4) # collect 4 steps for each training iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YeGuD1GcdGqC"
      },
      "outputs": [],
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "collect_driver.run = common.function(collect_driver.run)\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(train_env, agent.policy, num_episodes=1)\n",
        "returns = np.array([avg_return])\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = None\n",
        "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RqUTYoVdPI0",
        "outputId": "f4ca8047-6d53-41b7-9247-bc8b805a4731"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11388.7], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSfMTrzFdYld",
        "outputId": "4f10a6ef-5836-4f96-d61c-0ece6a2c3100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            " step 5000step = 5000: loss = 1529528320.0\n",
            "step = 5000: Average Return = 14054.251953125\n",
            " step 10000step = 10000: loss = 4072367104.0\n",
            "step = 10000: Average Return = 27981.68359375\n",
            " step 15000step = 15000: loss = 9688755200.0\n",
            "step = 15000: Average Return = 27981.68359375\n",
            " step 20000step = 20000: loss = 27174162432.0\n",
            "step = 20000: Average Return = 27981.68359375\n",
            " step 25000step = 25000: loss = 14264497152.0\n",
            "step = 25000: Average Return = 0.0\n",
            " step 30000step = 30000: loss = 36312207360.0\n",
            "step = 30000: Average Return = 19444.35546875\n",
            " step 35000step = 35000: loss = 48216670208.0\n",
            "step = 35000: Average Return = 19444.35546875\n",
            " step 40000step = 40000: loss = 80179060736.0\n",
            "step = 40000: Average Return = 21100.3671875\n",
            " step 45000step = 45000: loss = 70079340544.0\n",
            "step = 45000: Average Return = 19444.35546875\n",
            " step 50000step = 50000: loss = 173067796480.0\n",
            "step = 50000: Average Return = 14054.251953125\n",
            " step 55000step = 55000: loss = 186092617728.0\n",
            "step = 55000: Average Return = 14054.251953125\n",
            " step 60000step = 60000: loss = 189017751552.0\n",
            "step = 60000: Average Return = 0.0\n",
            " step 65000step = 65000: loss = 198138658816.0\n",
            "step = 65000: Average Return = 27981.68359375\n",
            " step 70000step = 70000: loss = 315413102592.0\n",
            "step = 70000: Average Return = 27981.68359375\n",
            " step 75000step = 75000: loss = 224302596096.0\n",
            "step = 75000: Average Return = 0.0\n",
            " step 80000step = 80000: loss = 402041634816.0\n",
            "step = 80000: Average Return = 0.0\n",
            " step 85000step = 85000: loss = 386915827712.0\n",
            "step = 85000: Average Return = 0.0\n",
            " step 90000step = 90000: loss = 358562332672.0\n",
            "step = 90000: Average Return = 0.0\n",
            " step 95000step = 95000: loss = 353172324352.0\n",
            "step = 95000: Average Return = 0.0\n",
            " step 100000step = 100000: loss = 370842730496.0\n",
            "step = 100000: Average Return = 21471.177734375\n",
            " step 100001"
          ]
        }
      ],
      "source": [
        "num_iterations = 100_000\n",
        "while True:\n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "    print(f'\\r step {step}', end='')\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        avg_return = compute_avg_return(train_env, agent.policy, num_episodes=1)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "        returns = np.append(returns, avg_return)\n",
        "\n",
        "    # if step % save_interval == 0:\n",
        "    #     save_checkpoint_to_local()\n",
        "\n",
        "    if step > num_iterations:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-osKaDQdclo",
        "outputId": "fd0f5fff-b1cd-4cd4-e7a4-9a8e04ed9bf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11388.7  , 14054.252, 27981.684, 27981.684, 27981.684,     0.   ,\n",
              "       19444.355, 19444.355, 21100.367, 19444.355, 14054.252, 14054.252,\n",
              "           0.   , 27981.684, 27981.684,     0.   ,     0.   ,     0.   ,\n",
              "           0.   ,     0.   , 21471.178], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M4vR0beHJuS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TFAgents-StockTrader.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}